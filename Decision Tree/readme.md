# Decision Tree , Random Forest, and AdaBoost Classifier

Decision Trees are a popular machine learning algorithm used for both classification and regression tasks.They create a tree-like model of decisions and their possible consequences. Each internal node represents a decision based on a feature, and each leaf node represents an outcome. Decision Trees are easy to interpret and can handle both numerical and categorical data.
Random Forest is an ensemble learning method based on Decision Trees. It builds multiple Decision Trees during training, each using a random subset of the data and features. The final prediction is made by averaging or voting on the predictions of individual trees. Random Forests are known for their high accuracy, resistance to overfitting, and robustness.
AdaBoost is an ensemble technique that combines multiple weak classifiers into a strong classifier. Weak classifiers are trained sequentially, with each one focusing on the mistakes of the previous ones. AdaBoost assigns weights to the training samples, giving more weight to misclassified samples. The final prediction is a weighted combination of the weak classifiers. AdaBoost is effective for binary classification tasks and can improve performance even with simple base classifiers.


