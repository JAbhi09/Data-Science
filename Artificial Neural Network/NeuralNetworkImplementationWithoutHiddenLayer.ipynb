{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMDMQx5ZhkgeBzyZjqL4J1z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Forward Propagation"],"metadata":{"id":"sXg6cGrMIoXb"}},{"cell_type":"markdown","source":["Import Library"],"metadata":{"id":"ne3fOeRFi6ll"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"B_w_yK52EcMV"},"outputs":[],"source":["import numpy as np"]},{"cell_type":"markdown","source":["Creating input data and output labels (basically AND operation)"],"metadata":{"id":"FL3tsBEWjK1A"}},{"cell_type":"code","source":["X = np.array([[0,0], [0,1], [1,0], [1,1]])\n","Y = np. array([[0,0,0,1]]).T"],"metadata":{"id":"Exs1446lFz5D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X.shape, Y.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8D-f4N5gGLb3","executionInfo":{"status":"ok","timestamp":1693025146711,"user_tz":-330,"elapsed":8,"user":{"displayName":"Abhishek Jha","userId":"17357565195979367948"}},"outputId":"dd9481c2-9097-4103-b0a7-8c786c397fd5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((4, 2), (4, 1))"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["Compute the sigmoid function"],"metadata":{"id":"Z301MJNHjk3Y"}},{"cell_type":"code","source":["def sig(z):\n","  return (1/ (1 + np.exp(-z)))"],"metadata":{"id":"jum5z7spGNuQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Compute the derivation of sigmoid function"],"metadata":{"id":"EfOVZ5Tujn--"}},{"cell_type":"code","source":["def derivation_sig(z):\n","  return sig(z) * (1 - sig(z))"],"metadata":{"id":"8LTRedWoOoPm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Initializing Weights and Biases"],"metadata":{"id":"aMxIQ2UVkRZC"}},{"cell_type":"code","source":["# Initializing weights and biases for the neural network's single neuron(No hidden layer)\n","# Weights are randomly initialized between -1 and 1\n","\n","\n","weights = 2 * np.random.random((2,1)) - 1\n","bias = 2 * np.random.random((1)) - 1\n","lr = 0.1\n","\n","print(\"Initialized Weights:\\n\", weights)\n","print(\"Initialized Bias:\\n\", bias)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SufxwavdGiQA","executionInfo":{"status":"ok","timestamp":1693026059702,"user_tz":-330,"elapsed":513,"user":{"displayName":"Abhishek Jha","userId":"17357565195979367948"}},"outputId":"e2c2c448-7b2c-4f00-b084-2dc1bd126e16"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Initialized Weights:\n"," [[0.77305843]\n"," [0.01937123]]\n","Initialized Bias:\n"," [0.45052166]\n"]}]},{"cell_type":"code","source":["# Forward propagation and weight updates for a single-neuron neural network(No hidden layer)\n","# Here we don't have any hidden layer so effectively our input layer of output is X.\n","\n","for iter in range(15000):\n","\n","  output0 = X\n","\n","  # Output prediction using sigmoid activation\n","  act_output = sig(np.dot(output0, weights) + bias)\n","\n","# Error terms calculation\n","\n","  # first term is (y_true-y_pred)\n","  first_term = act_output - Y\n","\n","  # second term is (o/p of j unit)* (1 - o/p of j unit) so, we're using der_sig\n","  second_term = derivation_sig(act_output)\n","\n","  first_two = first_term * second_term\n","\n","# Weight updates using calculated changes\n","  # changes that we want to apply on weights so we have the dimension of channges as the dimension of weights(2,1)\n","\n","  changes = np.array([[0.0], [0.0]])\n","\n","  for i in range(2):\n","    for j in range(4):\n","        changes[i][0] += first_two[j][0] * output0[j][i]\n","\n","  weights = weights - lr * changes\n","\n","# Bias update\n","  # Now for bais_change the only part is change that the o/p(output0[j][i]) is 1\n","\n","  bias_change = 0.0\n","  for j in range(4):\n","        bias_change += first_two[j][0] * 1\n","\n","  bias = bias - lr * bias_change\n","\n","# Final output prediction using trained weights and bias\n","final_output = sig(np.dot(X, weights) + bias)\n","\n","print(\"Updated Weights:\\n\", weights)\n","print(\"Updated Bias:\\n\", bias)\n","print(\"Final Output Predictions:\\n\", final_output)\n","print(\"Act Output Predictions:\\n\", act_output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GXw_7dF1GuFR","executionInfo":{"status":"ok","timestamp":1693027973667,"user_tz":-330,"elapsed":981,"user":{"displayName":"Abhishek Jha","userId":"17357565195979367948"}},"outputId":"64a44de8-0890-454e-83d0-7a446bc4ed96"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Updated Weights:\n"," [[9.41614807]\n"," [9.41614807]]\n","Updated Bias:\n"," [-14.41245428]\n","Final Output Predictions:\n"," [[5.50491219e-07]\n"," [6.71745222e-03]\n"," [6.71745224e-03]\n"," [9.88107010e-01]]\n","Act Output Predictions:\n"," [[5.50546697e-07]\n"," [6.71767637e-03]\n"," [6.71767639e-03]\n"," [9.88106615e-01]]\n"]}]},{"cell_type":"code","source":["# Forward propagation and weight updates for a single-neuron neural network(No hidden layer)\n","# Here we don't have any hidden layer so effectively our input layer of output is X.\n","\n","for iter in range(15000):\n","\n","  output0 = X\n","\n","  # Output prediction using sigmoid activation\n","  act_output = sig(np.dot(output0, weights) + bias)\n","\n","# Error terms calculation\n","\n","  # first term is (y_true-y_pred)\n","  first_term = act_output - Y\n","\n","  # second term is (o/p of j unit)* (1 - o/p of j unit) so, we're using der_sig\n","  second_term = derivation_sig(act_output)\n","\n","  first_two = first_term * second_term\n","\n","# Weight updates using calculated changes\n","\n","  # Here instead of for loop we're using matrix mul\n","  # so, we're doing dot product (dim_output0 is (4,2) and transpose of it is (2,4) and dim_first_two is (4,1) -----> ((2,1))\n","\n","  changes = np.dot(output0.T, first_two)\n","  weights = weights - lr * changes\n","\n","# Bias update\n","  # Now for bais_change the only part is changing that is o/p(output0[j][i]) which is equal 1 so, we can simply add up the first_two element instead of using for loop\n","\n","\n","  bias_change = np.sum(first_two)\n","  bias = bias - lr * bias_change\n","\n","# Final output prediction using trained weights and bias\n","final_output = sig(np.dot(X, weights) + bias)\n","\n","print(\"Updated Weights:\\n\", weights)\n","print(\"Updated Bias:\\n\", bias)\n","print(\"Final Output Predictions:\\n\", final_output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WhjcSRSxLjsl","executionInfo":{"status":"ok","timestamp":1693026063213,"user_tz":-330,"elapsed":1089,"user":{"displayName":"Abhishek Jha","userId":"17357565195979367948"}},"outputId":"d0328b20-0c5e-4dc1-bae9-8eb6b4c23743"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Updated Weights:\n"," [[8.01949595]\n"," [8.01949586]]\n","Updated Bias:\n"," [-12.31741562]\n","Final Output Predictions:\n"," [[4.47313930e-06]\n"," [1.34144208e-02]\n"," [1.34144221e-02]\n"," [9.76375806e-01]]\n"]}]},{"cell_type":"markdown","source":["**Conclusion:**\n","\n","A simple implementation of training a single-neuron neural network(No hidden layer) using forward propagation and gradient descent. This approach is suitable for solving binary classification problems where the output can be either 0 or 1."],"metadata":{"id":"KRCbptArlRSs"}},{"cell_type":"code","source":[],"metadata":{"id":"PdGSR0NxIdPK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IB2YN_yyJQJX","executionInfo":{"status":"ok","timestamp":1692972632278,"user_tz":-330,"elapsed":9,"user":{"displayName":"Abhishek Jha","userId":"17357565195979367948"}},"outputId":"27adc92f-83c9-44e8-b983-3bf889817317"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.37639952],\n","       [0.36453838],\n","       [0.38962847],\n","       [0.38014603]])"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":[],"metadata":{"id":"tRB-nkHeK47x"},"execution_count":null,"outputs":[]}]}