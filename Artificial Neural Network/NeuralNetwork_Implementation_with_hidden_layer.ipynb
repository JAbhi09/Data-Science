{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP/pxd2qU5p6XBarSbGpvJ6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Import Library"],"metadata":{"id":"eHT1txszr_ar"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"S7fN0Vfxbd2x"},"outputs":[],"source":["import numpy as np"]},{"cell_type":"markdown","source":["Input and Output Data Initialization"],"metadata":{"id":"ysqGZ1Z4seDu"}},{"cell_type":"code","source":["# Define the input data matrix and output data matrix that is actually a XOR function.\n","\n","X = np.array([[0,0], [0,1], [1,0], [1,1]])\n","Y = np. array([[0,1,1,0]]).T"],"metadata":{"id":"bpDIaJb6bozf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X.shape, Y.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cLddJemMbqo7","executionInfo":{"status":"ok","timestamp":1693032216352,"user_tz":-330,"elapsed":4,"user":{"displayName":"Abhishek Jha","userId":"17357565195979367948"}},"outputId":"e0b08dea-0328-406a-d42f-ad6f94fa3910"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((4, 2), (4, 1))"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","source":["Calculate the sigmoid activation function."],"metadata":{"id":"16VPPzotsoHI"}},{"cell_type":"code","source":["def sig(z):\n","  return (1/ (1 + np.exp(-z)))"],"metadata":{"id":"g1p-YalzbrO-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Calculate the derivative of the sigmoid activation function."],"metadata":{"id":"g-fzcd3DsvFP"}},{"cell_type":"code","source":["def derivation_sig(z):\n","  return sig(z) * (1 - sig(z))"],"metadata":{"id":"UbCL5nXgOSVi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#  Suppose we have one input layer(with 2 fea and 1 bais), one hidden layer (with 2 node and on bais node) and one output layer\n","\n","# Initialize weights and biases for the hidden layer.\n","hid_weight = 2 * np.random.random((2,2)) - 1\n","hid_bias = 2 * np.random.random((2)) - 1\n","\n","# Initialize weights and biases for the output layer.\n","output_weight = 2 * np.random.random((2,1)) - 1\n","output_bias = 2 * np.random.random((1,1)) - 1\n","lr = 0.1"],"metadata":{"id":"vGIGHmSjbsH2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"936It0B3tKyd"}},{"cell_type":"code","source":["# Perform forward and backward propagation to train the neural network.\n","\n","\n","for iter in range(10000):\n","\n","  # Forward Propagation\n","  output0 = X\n","  input_hidden = np.dot(output0, hid_weight) + hid_bias\n","  output_hid = sig(input_hidden)\n","  inputforOutputLayer = np.dot(output_hid, output_weight) + output_bias\n","  act_output = sig(inputforOutputLayer)\n","\n","# Calculate the error and its gradient for the output layer.\n","  first_term_op_layer = act_output - Y\n","  second_term_op_layer = derivation_sig(inputforOutputLayer)\n","  first_two_op_layer = first_term_op_layer * second_term_op_layer\n","\n","# Calculate the error and its gradient for the hidden layer.\n","  first_term_hid_layer = np.dot(first_two_op_layer, output_weight.T)\n","  second_term_hid_layer = derivation_sig(input_hidden)\n","  first_two_hid_layer = first_term_hid_layer * second_term_hid_layer\n","\n","# Calculate weight and bias adjustments for the output layer.\n","  changes_op_layer = np.dot(output_hid.T, first_two_op_layer)\n","  changes_op_layer_bias = np.sum(first_two_op_layer, axis=0, keepdims=True)\n","\n","# Calculate weight and bias adjustments for the hidden layer.\n","  changes_hid_layer = np.dot(output0.T, first_two_hid_layer)\n","  changes_hid_layer_bias = np.sum(first_two_hid_layer, axis = 0, keepdims=True)\n","\n","# Update weights and biases using gradient descent.\n","  output_weight = output_weight - lr*changes_op_layer\n","  output_bias = output_bias - lr*changes_op_layer_bias\n","  hid_weight = hid_weight - lr*changes_hid_layer\n","  hid_bias = hid_bias - lr*changes_hid_layer_bias\n","\n","\n","# Perform final predictions after training.\n","output0 = X\n","input_hidden = np.dot(output0, hid_weight) + hid_bias\n","output_hid = sig(input_hidden)\n","inputforOutputLayer = np.dot(output_hid, output_weight) + output_bias\n","act_output = sig(inputforOutputLayer)\n","\n","\n","# Print updated weights, biases, and final predictions.\n","print(\"Updated Hidden Layer Weights:\\n\", hid_weight)\n","print(\"Updated Hidden Layer Bias:\\n\", hid_bias)\n","print(\"Updated Output Layer Weights:\\n\", output_weight)\n","print(\"Updated Output Layer Bias:\\n\", output_bias)\n","print(\"Final Output Predictions:\\n\", act_output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ijKYwQuQbr74","executionInfo":{"status":"ok","timestamp":1693032265990,"user_tz":-330,"elapsed":977,"user":{"displayName":"Abhishek Jha","userId":"17357565195979367948"}},"outputId":"c012139d-5618-4ff3-cb0e-9cbc085a91d0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Updated Hidden Layer Weights:\n"," [[ 5.03522527 -5.13966857]\n"," [-5.21294654  4.94755702]]\n","Updated Hidden Layer Bias:\n"," [[-2.86629121 -2.81152551]]\n","Updated Output Layer Weights:\n"," [[7.24181237]\n"," [7.26012007]]\n","Updated Output Layer Bias:\n"," [[-3.57147279]]\n","Final Output Predictions:\n"," [[0.05897514]\n"," [0.94901555]\n"," [0.94931286]\n"," [0.05220239]]\n"]}]},{"cell_type":"code","source":["# Compare Ground Truth Labels with Predicted Output\n","true_labels = np.array([[0, 1, 1, 0]]).T\n","print(\"Ground Truth Labels:\")\n","print(true_labels)\n","\n","print(\"Predicted Output:\")\n","print(act_output)\n","\n","print(\"The predicted output values have been compared to the ground truth labels to assess the model's accuracy.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"REw6d-WivRtK","executionInfo":{"status":"ok","timestamp":1693034935301,"user_tz":-330,"elapsed":453,"user":{"displayName":"Abhishek Jha","userId":"17357565195979367948"}},"outputId":"de153873-721f-4db8-df41-e9853440f31c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Ground Truth Labels:\n","[[0]\n"," [1]\n"," [1]\n"," [0]]\n","Predicted Output:\n","[[0.05897514]\n"," [0.94901555]\n"," [0.94931286]\n"," [0.05220239]]\n","The predicted output values have been compared to the ground truth labels to assess the model's accuracy.\n"]}]},{"cell_type":"markdown","source":["**Conclusion:**\n","\n","In this project, a neural network was developed from scratch, including architecture setup, activation functions, forward and backward propagation. Through 10000 training iterations, the network learned to approximate the target output, demonstrating the fundamental principles of neural network training."],"metadata":{"id":"lU3OoSJSuXNT"}},{"cell_type":"code","source":[],"metadata":{"id":"EYxU_ohTvBw8"},"execution_count":null,"outputs":[]}]}